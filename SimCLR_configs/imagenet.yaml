# default configuration with all args specified

# distributed training/slurm arguments
nodes: 16
gpus: 4 # I recommend always assigning 1 GPU to 1 node
node_rank: 0 # machine nr. in node (0 -- nodes - 1)
local_rank: -1  # range: (0 -- num_gpus_per_node - 1)
dataparallel: 0 # Use DataParallel instead of DistributedDataParallel
ddp: True # Use DistributedDataParallel
workers: 16
timeout: 3600
partition: "v100*"

# SLURM/Submitit Arguments
slurm_job_name: "simclr"
comment: "SimCLR run" # for slurm
job_dir: "slurm_log/simclr_%j"  # log_ddp/%j
master_addr: "127.0.0.1"  # this is the value used in Janne's implementation
walltime: 2880  # in minutes (max walltime on Longhorn is 2,880 minutes)

# train options
seed: 42 # sacred handles automatic seeding when passed in the config
batch_size: 64 # https://discuss.pytorch.org/t/do-dataparallel-and-distributeddataparallel-affect-the-batch-size-and-gpu-memory-consumption/97194
image_size: 224
start_epoch: 0
epochs: 250
dataset: "MSLUnlabeled"
dataset_dir: "~/clover_shared/datasets/combined_dataset_09-13-2022/Imagenet
train_file_map: ""
test_file_map: ""
use_file_map: True
pretrain: False
num_classes: 19
num_images: 1281167  # used in UnlabeledImageDataset; default value matches MSL2.1
use_wandb: True  # whether to use weights and biases
tensorboard_log_dir: null  # None by default
wandb_project: "msl_2M_OneCycleLR_test"  # WandB project name; unused if use_wandb = False
wandb_run_id: null  # used to resume a previous wandb run
wandb_preview_imgs: False

# model options
resnet: "r50_1x_sk0"
num_projection_layers: 3
projection_dim: 128 # "[...] to project the representation to a 128-dimensional latent space"
# Which layer of the projection head to use during fine-tuning. 
# Goes from [0, num_projection_layers],
# where 0 is the encoder output or input into the contrastive head
projection_output_layer: 0  

# loss options
optimizer: "LARS" # or LARS (experimental)
base_learning_rate: 0.075 # Applies to LARS Optimizer. 0.3/0.075 value from Google Research
weight_decay: 1.0e-6 # "optimized using LARS [...] and weight decay of 10âˆ’6" try 1E-4
temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes
learning_rate_scaling: "sqrt" # "linear" or "sqrt"
learning_rate_scheduler: "Cosine" # "Cosine" or "OneCycle"
nt_xent_forward_method: "google"

# reload options
model_path: "/work/08452/kaipak/longhorn/model_output/msl_2M_pretrain_refactor" # set to the directory containing `checkpoint_##.tar`
epoch_num: 150 # set to checkpoint number
reload: False

# logistic regression (linear eval) options
reload_during_eval: True # whether to load checkpoint during eval
# if reload_during_eval = False and pretrain = False, a randomly initialized model will be evaluated
# if reload_during_eval = False and pretrain = /path/to/r50_1x_sk0.pth, the pretrained model will be evaluated
logistic_batch_size: 256
logistic_epochs: 500
early_stopping: False
patience: 7  # patience for early stopping
save_features: False
reuse_features: False
save_ext: 'csv'  # parquet or csv; parquet with pyarrow is faster
zero_shot_eval: False
