# default configuration with all args specified
defaults: [{ "": "default_config.yaml" }]
# distributed training/slurm arguments
nodes: 2
gpus: 3 # I recommend always assigning 1 GPU to 1 node
node_rank: 0 # machine nr. in node (0 -- nodes - 1)
local_rank: -1  # range: (0 -- num_gpus_per_node - 1)
dataparallel: 0 # Use DataParallel instead of DistributedDataParallel
ddp: True # Use DistributedDataParallel
workers: 16
timeout: 3600
partition: "gpu-a100"
# SLURM/Submitit Arguments
slurm_job_name: "msl_pre_simclr"
comment: "SimCLR run" # for slurm
job_dir: "slurm_log/simclr_%j"  # log_ddp/%j
master_addr: "127.0.0.1"  # this is the value used in Janne's implementation
walltime: 2880  # in minutes (max walltime on Longhorn is 2,880 minutes)

# train options
seed: 42 # sacred handles automatic seeding when passed in the config
batch_size: 171 # https://discuss.pytorch.org/t/do-dataparallel-and-distributeddataparallel-affect-the-batch-size-and-gpu-memory-consumption/97194
image_size: 224
start_epoch: 0
epochs: 100
dataset: "Mars2020"
dataset_dir: "/scratch/08452/kaipak/datasets/msl_images"
train_file_map: ""
test_file_map: ""
use_file_map: True
pretrain: False
num_classes: 19
num_images: -1  # used in UnlabeledImageDataset; default value matches MSL2.1
use_wandb: True  # whether to use weights and biases
tensorboard_log_dir: null  # None by default
wandb_project: "SimCLR_refactor_tests_MSL"  # WandB project name; unused if use_wandb = False
wandb_run_id: null  # used to resume a previous wandb run
wandb_preview_imgs: False

# clustering
clustering_support_size: 8
clustering_false_negative_sim_threshold: 0.7
enable_clustering_training: True
cluster_file_map: "/home1/08452/kaipak/clover_shared/clustered.csv"

# model options
resnet: "r50_1x_sk0"
num_projection_layers: 3
projection_dim: 128 # "[...] to project the representation to a 128-dimensional latent space"
# Which layer of the projection head to use during fine-tuning. 
# Goes from [0, num_projection_layers],
# where 0 is the encoder output or input into the contrastive head
projection_output_layer: 0
use_cifar_mod: False

# loss options
optimizer: "LARS_lf" # or LARS (experimental)
base_learning_rate: 0.075 # Applies to LARS Optimizer. 0.3/0.075 value from Google Research
weight_decay: 1.0e-4 # "optimized using LARS [...] and weight decay of 10âˆ’6" try 1E-4
temperature: 0.2 # see appendix B.7.: Optimal temperature under different batch sizes
learning_rate_scaling: "linear" # "linear" or "sqrt"
learning_rate_scheduler: "OneCycle" # "Cosine" or "OneCycle"
cosine_annealing_eta_min: 0 # Default 0. Only applicable if learning_rate_scheduler is Cosine
onecycle_div_factor: 25.0 # Default 25.0. Applicable for learning_rate_scheduler = OneCycle
onecycle_final_div_factor: 10000.0 # Default 10000.0. Applicable for learning_rate_scheduler = OneCycle
nt_xent_forward_method: "clustering"

# reload options
model_path: "/work/08452/kaipak/ls6/model_output/msl_2M_refactor_test2" # set to the directory containing `checkpoint_##.tar`
epoch_num: 99 # set to checkpoint number
reload: False

# logistic regression (linear eval) options
reload_during_eval: True # whether to load checkpoint during eval
# pretrain: "/work/08452/kaipak/ls6/model_output/simclr_pretrained/r50_1x_sk0.pth"
# if reload_during_eval = False and pretrain = False, a randomly initialized model will be evaluated
# if reload_during_eval = False and pretrain = /path/to/r50_1x_sk0.pth, the pretrained model will be evaluated
logistic_learning_rate: 0.0003
logistic_batch_size: 512
logistic_epochs: 100
#logistic_checkpoints: [49, 99, 149, 199, 249, 299, 349, 399, 449, 499]
#logistic_checkpoints: [9, 49, 99, 149, 199, 249, 299, 349, 399, 449, 499]
logistic_checkpoints: [9, 19, 29, 39, 49, 59, 69, 79, 89, 99]
logistic_pcts: [0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1.0]
early_stopping: False
patience: 7  # patience for early stopping
save_features: False
reuse_features: False
save_ext: 'csv'  # parquet or csv; parquet with pyarrow is faster
zero_shot_eval: False
